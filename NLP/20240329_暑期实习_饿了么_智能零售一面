1.Transformer和BERT的区别，各自架构长什么样子。
  这应该很简单。
2.BERT的输入，遮蔽掩码建模和next to seq方式。
3.BERT的掩码技术的三种实现方式。
  BERT（Bidirectional Encoder Representations from Transformers）模型在预训练过程中使用了掩码技术（Masked Language Model，MLM）来训练模型，使其能够理解句子中缺失的词语或词语的上下文。
  掩码技术的三种实现方式主要包括：
  1. **随机掩码（Random Masking）**：
     在这种方式下，BERT模型会随机选择一定比例的词语，并将其替换为特定的掩码符号（如 `[MASK]`）。然后，模型尝试根据上下文信息来预测被掩码的词语。这种方式不依赖于任何特定的规则，而是完全随机地选择要掩码的词语，使得模型在预训练时能够适应各种不同的掩码情况。
  2. **连续片段掩码（Continuous Segment Masking）**：
     这种方式是为了模拟一些语言中连续词语的缺失情况，例如英文中的连续词语缩写或者断句等情况。在这种方式下，BERT模型会选择一些连续的片段，并将这些片段中的词语替换为掩码符号。然后，模型尝试根据上下文信息来预测被掩码的词语。这种方式可以更好地模拟实际语言中的缺失情况，提高模型在处理连续片段缺失时的鲁棒性。
  3. **特定词语掩码（Special Token Masking）**：
     这种方式是为了模拟一些语言中特定词语的缺失情况，例如英文中的人名、地名或专有名词等。在这种方式下，BERT模型会选择一些特定的词语，并将这些词语替换为掩码符号。然后，模型尝试根据上下文信息来预测被掩码的词语。这种方式可以更好地模拟实际语言中的特定词语缺失情况，提高模型对这些词语的理解能力。
  这些掩码技术的目的都是为了使BERT模型能够适应不同类型的词语缺失情况，并学习到更加丰富和鲁棒的语言表示。在预训练过程中，这些技术能够有效地提高模型的性能和泛化能力。

4.自注意力怎么实现的
5.多头自注意力和普通自注意力差别，计算效率上相比呢？
  多头自注意力（Multi-Head Self-Attention）和普通自注意力（Single-Head Self-Attention）在计算效率和表达能力上有一些区别。
  1. **表达能力**：
     - 普通自注意力：普通自注意力计算一个加权的上下文向量，该向量是所有输入序列位置的加权和，其中权重是通过计算查询、键和值之间的相似度得到的。这个上下文向量会被用于计算最终的输出。
     - 多头自注意力：多头自注意力通过将查询、键和值投影到不同的子空间（即头）中，并对每个子空间进行独立的自注意力计算，然后将所有子空间的输出拼接起来并进行线性变换。这样做可以增加模型的表达能力，因为每个子空间可以学习到不同的特征表示。
  2. **计算效率**：
     - 普通自注意力：在普通自注意力中，对于输入序列长度为 \(n\) 的情况，计算复杂度为 \(O(n^2)\)，因为每个位置都要计算与其他所有位置的相似度。
     - 多头自注意力：在多头自注意力中，假设有 \(h\) 个头，每个头的维度为 \(d_h\)，则计算复杂度为 \(O(n \cdot d_h^2 \cdot h)\)，因为对于每个头，需要计算查询、键和值的投影，并且对于每个位置需要计算与其他位置的相似度。尽管计算复杂度相比普通自注意力有所增加，但是由于每个头的维度通常比整个注意力机制的维度低，因此实际上多头自注意力通常更高效。
  总的来说，多头自注意力相比普通自注意力在表达能力上更加强大，同时在实际计算中通常也更加高效。因此，在现代的深度学习模型中，多头自注意力经常被广泛应用，特别是在Transformer等模型中。

6.为什么Transformer结构用Layer Norm不用传统的Batch Norm？
  Transformer 结构使用 Layer Normalization（Layer Norm）而不是传统的 Batch Normalization（Batch Norm）的原因有几个：
  1. **独立性**：
     Batch Norm 是对每个输入特征的整个批次进行归一化，因此它依赖于批次中的所有样本。这意味着在训练时，每个样本都受到整个批次中其他样本的影响，导致了输入之间的相关性。而 Layer Norm 是对每个样本的特征进行归一化，每个样本都有独立的均值和方差，因此不会受到其他样本的影响，保持了输入特征的独立性。
  2. **适应性**：
     Batch Norm 在训练和推断阶段的行为不同，需要在推断阶段保持批次统计信息。这对于序列模型（如 Transformer）来说可能会有问题，因为在推断阶段需要逐步处理序列，无法直接使用整个批次的统计信息。而 Layer Norm 在训练和推断阶段的行为是一致的，不需要保持批次统计信息，因此更适合序列模型的训练和推断。
  3. **稳定性**:
     Batch Norm 对输入特征的归一化是基于批次统计信息计算的，这可能会导致训练过程中的不稳定性，特别是在批次大小较小时。而 Layer Norm 是基于样本特征的独立统计信息计算的，更稳定，对批次大小的变化不敏感，有利于训练的稳定性和收敛性。
  4. **位置信息**：
     在自然语言处理任务中，位置编码是非常重要的，而 Batch Norm 会破坏位置编码的信息，因为它将所有位置的信息整合到了批次中进行归一化。相比之下，Layer Norm 是对每个样本的特征进行归一化，保留了位置信息。

7.BERT进行遮蔽掩码建模的概率选择策略？常用0.15，为什么？
8.BERT训练过程？
9.LLAM2在Transformer架构上进行了哪些改进？
 可以从激活函数改进SWIGLU，layernorm改进resnorm，norm在前，复数的旋转位置编码，稀疏注意力，多查询注意力，再有就是FLASH ATTENTIUON和显卡训练底层优化。
10.BERT是怎么由word2vec模型过来的？二者有哪些相关性？或者说BERT借鉴了word2vec哪些思想？

算法题：
1.设计一个随机采样的函数。
a = [(1,0.4),(2,0.5),(3,0.1)]
p = random() 值域[0, 1] 根据p在a中采样

思路：1.二分（简单版本） 2.考虑底层实现，可看pytorch源码

2.投骰子
A:[1 - 20] 一枚1 - 20点数的骰子
B:[3 * [1 - 6]] 1 - 6的骰子投掷三次取sum

问p(A > B) 和 p(A < B)的关系。
二者应该一样。。考虑独立分布的期望线性可加行，以及均值左右的对称分布特点。概率论。
