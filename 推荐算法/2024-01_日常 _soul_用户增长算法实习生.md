### soul用户增长算法实习生
---
#### 基础信息
时间:35分钟  
网上面试

#### 一面
##### 面试部分
###### 介绍一下自己的项目经历

回答概括： 分别介绍了自己的两个kaggle比赛，并且介绍了其中的数据分析与特征工程的部分，其中使用的主要模型为xgboost与lightgbm.

###### 介绍一下树模型与深度模型的区别与优缺点
回答：
树模型优点：
- 实现简单方便作为思路验证
- 可解释性强可以用于一些强解释的场景

深度模型优点：
- 在数据量较大的情况下往往深度学习模型效果更佳优秀

使用场景：
- 数据量较大 深度模型
- 数据量较小或者解释性强的领域(风控领域) 树模型
- 希望快速验证想法 树模型

###### 介绍一下xgboost与lightgbm的区别

回答 ： 
- 生长策略
- 直方图算法
- 并行优化策略
- Cache策略

参考答案:
- 树生长策略：XGB采用level-wise的分裂策略，LGB采用leaf-wise的分裂策略。XGB对每一层所有节点做无差别分裂，但是可能有些节点增益非常小，对结果影响不大，带来不必要的开销。Leaf-wise是在所
叶子节点中选取分裂收益最大的节点进行的，但是很容易出现过拟合问题，所以需要对最大深度做限制 。
- 分割点查找算法：XGB使用特征预排序算法，LGB使用基于直方图的切分点算法，其优势如下：减少内存占用，比如离散为256个bin时，只需要用8位整形就可以保存一个样本被映射为哪个bin(这个bin可以说就是转换后的特征)，对比预排序的exact greedy算法来说（用int_32来存储索引+ 用float_32保存特征值），可以节省7/8的空间。计算效率提高，预排序的Exact greedy对每个特征都需要遍历一遍数据，并计算增益，复杂度为 (# ×# )。而直方图算法在建立完直方图后，只需要对每个特征遍历直方图即可，复杂度为 (# ×# )。LGB还可以使用直方图做差加速，一个节点的直方图可以通过父节点的直方图减去兄弟节点的直方图得到，从而加速计算但实际上xgboost的近似直方图算法也类似于lightgbm这里的直方图算法，为什么xgboost的近似算法比lightgbm还是慢很多呢？xgboost在每一层都动态构建直方图， 因为xgboost的直方图算法不是针对某个特定的feature，而是所有feature共享一个直方图(每个样本的权重是二阶导)，所以每一层都要重新构建直方图，而lightgbm中对每个特征都有一个直方图，所以构建一次直方图就够了。
- 支持离散变量：无法直接输入类别型变量，因此需要事先对类别型变量进行编码（例如独热编码），而LightGBM可以直接处理类别型变量。
- 缓存命中率：XGB使用Block结构的一个缺点是取梯度的时候，是通过索引来获取的，而这些梯度的获取顺序是按照特征的大小顺序的，这将导致非连续的内存访问，可能使得CPU cache缓存命中率低，从而影响算法效率。而LGB是基于直方图分裂特征的，梯度信息都存储在一个个bin中，所以访问梯度是连续的，缓存命中率高。
- LightGBM 与 XGboost 的并行策略不同：
    - 特征并行 ：LGB特征并行的前提是每个worker留有一份完整的数据集，但是每个worker仅在特征子集上进行最佳切分点的寻找；worker之间需要相互通信，通过比对损失来确定最佳切分点；然后将这个最佳切分点的位置进行全局广播，每个worker进行切分即可。XGB的特征并行与LGB的最大不同在于XGB每个worker节点中仅有部分的列数据，也就是垂直切分，每个worker寻找局部最佳切分点，worker之间相互通信，然后在具有最佳切分点的worker上进行节点分裂，再由这个节点广播一下被切分到左右节点的样本索引号，其他worker才能开始分裂。二者的区别就导致了LGB中worker间通信成本明显降低，只需通信一个特征分裂点即可，而XGB中要广播样本索引。
    - 数据并行 ：当数据量很大，特征相对较少时，可采用数据并行策略。LGB中先对数据水平切分，每个worker上的数据先建立起局部的直方图，然后合并成全局的直方图，采用直方图相减的方式，先计算样本量少的节点的样本索引，然后直接相减得到另一子节点的样本索引，这个直方图算法使得worker间的通信成本降低一倍，因为只用通信以此样本量少的节点。XGB中的数据并行也是水平切分，然后单个worker建立局部直方图，再合并为全局，不同在于根据全局直方图进行各个worker上的节点分裂时会单独计算子节点的样本索引，因此效率贼慢，每个worker间的通信量也就变得很大。
    - 投票并行（LGB）：当数据量和维度都很大时，选用投票并行，该方法是数据并行的一个改进。数据并行中的合并直方图的代价相对较大，尤其是当特征维度很大时。大致思想是：每个worker首先会找到本地的一些优秀的特征，然后进行全局投票，根据投票结果，选择top的特征进行直方图的合并，再寻求全局的最优分割点。

[参考链接](https://zhuanlan.zhihu.com/p/99069186)
###### 请问你都知道什么深度学习中正则化的方式
回答：
- l2正则化
- dropout
- batchNormal
- LayerNormal
- 一些规则化正则项

###### 如果我有一个batch-size为20480的网络现在我希望将batch-size修改为2048请问我该如何调整学习率
回答：
根据大数定律我们知道当数据量越大的情况下，模型越有可能接近理想的分布，因此当batch-size减小的时候分布的可信度将下降即更加不倾向于理想分布，因此应该采用更加谨慎的学习率。

##### 代码题

###### 请实现一个简单的线性分类模型(只能使用numpy库)


实现思路：
- 实现一个Tensor基础类用于存储权重与梯度
- 实现一个Layer基类接口 forward backward
- 实现 DNN SoftMax ReLu
- 实现一个基础的Optimizer 如果你不想实现的太复杂可以考虑仅仅完成基础的zero_grad和update 如果嫌麻烦可以直接使用字典存储
- 实现一个基础的工作流

